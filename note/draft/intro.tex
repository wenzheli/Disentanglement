A desired property among others for representation learning is to disentangle the factors of variations of the input data~\cite{bengio2013representation}.
For example, the natural image of a person is the composition of human identity, pose, illumination, etc. Successful separation of these factors naturally gives rise to some other important properties like invariance in representation learning~\cite{cohen2014learning}. If the prediction task is to recognize different persons from images, we only need examine the units of the representation controlling the identity factor and simply ignoring other units. Also, learning disentangled representations helps better understanding and/or visualization of the data.

The first challenging problem we are facing is \textit{how to define the disentanglement?} Suppose we already know the group assignments for each hidden unit (i.e., we assume each group of hidden units control one specific factor), then intuitively, we hope that the correlation among units within the same group is high while the correlation between groups is small (or 0 in ideal case). More formally, one way to model this intuition is to use block diagonal covariance matrices, where each block corresponds to one particular factor.

The second challenging problem is \textit{how to define the block structure of the covariance matrix?} Unfortunately, in general we do not have information on how hidden units group together. Nonetheless, we could pre-define the structure of block diagonal covariance matrix (i.e., the number of blocks and the size of each block) in some situations. This leads to our first solution. After presenting that, we extend it by letting our model automatically learn the structures from the data. For this purpose, we add the block diagonal generative process prior on the latent representations.


In summary, we make the following contributions:
\begin{itemize}
\item We propose a principled way to disentangle factors of variation. To the best of our knowledge, all the methods proposed before focus on only discovering two factors from the data by leveraging the labeling information and known block structures. In this paper, we generalize these methods to discover more factors completely in an unsupervised way. We also further generalize our approach to the case when the block structure is unknown, which has not been explored before to our best knowledge. By introducing the variable clustering prior~\cite{palla2012nonparametric}, our model can automatically discover the right number of factors from the data. Although the current solution assumes the number of blocks is known, it can be easily extended to the nonparametric case by using Dirichless process prior.
\item We conduct several experiments on three image datasets. We evaluate the performance of these models in terms of generated samples, predictive performance, correlation analysis and clustering plots. The results show that the proposed models are able to effectively discover different factors. In addition, we also find that the model trained with unknown block structure has comparable performance to those with known block structures.
\end{itemize}


\subsection{Related Work}
Learning disentangled representations has gained considerable interests from the deep learning community recently. Next we discuss and compare our approach with several existing models.

In \cite{reed2014learning}, the authors propose a higher-order Boltzmann machine that incorporates the multiplicative interactions between hidden units each of which learns to encode a distinct factor of variations. A similar work is proposed in \cite{cheung2014discovering}, where they built an autoencoder model to learn two factors of variation by setting the connection between a group of hidden units and data labels.

However, both models are limited in the following. They both rely on the labeling information, which in some cases is not readily available. Both of them lack the generalization capability for more than two factors. In particular, the autoencoder model in~\cite{cheung2014discovering} can only discover up to two factors. Although the model in \cite{reed2014learning} potentially can be extended to learn more than two factors, the computational cost increases exponentially with the exponential increase of the cross-penalty terms. Both models need to define the exact block structures, i.e., the number of blocks and the size of each block. This significantly limits the power of these models. Compared to the previous work, our methods work completely in an unsupervised way and is easily applicable to the data with more than two underlying factors, with unknown block structures.

More recently, \cite{cohen2014learning} provides a theoretical analysis of disentangled representation from the perspective of group theory, by leveraging the symmetry property. However, this model also suffers from the fact that they need to pre-define the exact block structures.


\iffalse
\subsection{Related Work}
The problem of learning disentangled representations have gained considerable interest from deep learning community recently, and there are few models have been proposed.

In \cite{reed2014learning}, they propose a higher-order Boltzmann machine that incorporates the multiplicative interactions between hidden units that each learns to encode distinct factor of variation. Another similar work is proposed in \cite{cheung2014discovering}, where they built an autoencoder model to learn two factors of variation by setting the connection between one group of hidden units and data labels.

However, both of the models have significant limitations: 1. They both reply on the label information, which in some cases are not readily available. 2. Both of them lack of generalization capability for more than two factors. In particular, the autoencoder model in~\cite{cheung2014discovering} can only discover two factors. Although the model in \cite{reed2014learning} potentially can be extended to learn more than two factors, the computational cost increases exponentially with the exponential increase of the cross penalty terms. 3. Both models needs to define the exact block structures, i.e the number of blocks, and the size of each block, which significantly limits the power of these models. Compared to these previous work, our methods work completely in an unsupervised way, and easily apply to the data with more than two factors, also with unknown block structures.

More recently, \cite{cohen2014learning} provides theoretical analysis of disentangled representation from the perspective of group theory, by levering the symmetry property. However, the setting also suffers from the fact that they need to pre-define the exact block structures.

\fi










