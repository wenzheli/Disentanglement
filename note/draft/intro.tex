One desired property for representation learning is to disentangle the factors of variation from the input data~\cite{bengio2013representation}.
For example, an image is the composition of different factors such as identity, illumination, brightness, pose, etc. Successful separation of these factors naturally brings in the property of invariance~\cite{cohen2014learning}. i.e if the prediction task is to classify image into different persons, then we can only examine the units controlling the identity factor while simply ignoring other units that control the nuisance factors. Also, learning disentangled representation helps better understanding of the data. \textcolor{blue}{Add more reasons why disentangling problem is important?}

The first challenging problem we are facing with is \textit{how to define the disentanglement?}. Suppose we already know the group assignments for each hidden unit (i.e we assume each group of hidden units control one specific factor), then intuitively, we hope that the correlation among units within the same group is high while the correlation between groups is small (or 0 in ideal case). More formally, one way to model this is using block diagonal covariance matrix, where each block corresponds to one particular factor.

The second challenging problem is \textit{how to define the block structure?}. In fact, defining block structure is equivalent to define the covariance structure. Unfortunately, in general, we don't have information on how hidden units group together. Nonetheless, we could still pre-define the structure of block diagonal covariance matrix (i.e number of blocks, and the size of each block), which leads to our first solution. But ideally, we hope that our model can automatically learn the structures from the data. For this purpose, we add the block diagonal generative process prior on the latent representations.


In summary, we make the following contributions:
\begin{itemize}
\item We propose a principled way to disentangle factors of variation. To the best of our knowledge, all the methods proposed before focus only discovering two factors from the data by leveraging the label information and known block structures. In this paper, we generalize these methods to discover more factors completely in an unsupervised way. We also further generalize our approach to the case when the block structure is unknown, which hasn't been explored before. By putting variable clustering prior~\cite{palla2012nonparametric}, our model can automatically discover the right number of factors from the data. Although the current solution assumes the number of blocks is known, it can be easily extended to the nonparametric case by using Dirichless process prior.
\item We conduct several experiments on three image datasets. We evaluate the performance of these models in terms of generated samples, predictive performance, correlation analysis and clustering plots. The results show that the proposed models are able to effectively discover different factors. In addition, we also find that the model trained with unknown block structure has comparable performance to the ones trained with known block structures.
\end{itemize}


\subsection{Related Work}
The problem of learning disentangled representations have gained considerable interest from deep learning community recently, and there are few models have been proposed.

In \cite{reed2014learning}, they propose a higher-order Boltzmann machine that incorporates the multiplicative interactions between hidden units that each learns to encode distinct factor of variation. Another similar work is proposed in \cite{cheung2014discovering}, where they built an autoencoder model to learn two factors of variation by setting the connection between one group of hidden units and data labels.

However, both of the models have significant limitations: 1. They both reply on the label information, which in some cases are not readily available. 2. Both of them lack of generalization capability for more than two factors. In particular, the autoencoder model in~\cite{cheung2014discovering} can only discover two factors. Although the model in \cite{reed2014learning} potentially can be extended to learn more than two factors, the computational cost increases exponentially with the exponential increase of the cross penalty terms. 3. Both models needs to define the exact block structures, i.e the number of blocks, and the size of each block, which significantly limits the power of these models. Compared to these previous work, our methods work completely in an unsupervised way, and easily apply to the data with more than two factors, also with unknown block structures.

More recently, \cite{cohen2014learning} provides theoretical analysis of disentangled representation from the perspective of group theory, by levering the symmetry property. However, the setting also suffers from the fact that they need to pre-define the exact block structures.












