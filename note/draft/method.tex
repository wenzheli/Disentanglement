
\begin{figure}[hb]
  \centering
  \includegraphics[width=1.2in]{images/model.png}
  \caption[]
   {Graphical representation of proposed model. $\alpha$ encodes the set of parameters for prior on $\mathbf{z}$, $\theta$ encodes the parameters for the generative model $p(\mathbf{x}|\mathbf{z})$, $\phi$ are the parameters for approximate posterior $p(\mathbf{z}|\mathbf{x})$}
  \label{fig:model}
\end{figure}

\subsection{Problem Definition}
Suppose we have dataset $\{\mathbf{x}_i\}_{i=1}^{N}$ where $\mathbf{x}_i\in R^D$ and $N$ is the number of samples. The goal is to learn the hidden representation $\mathbf{z}_i\in R^K$ for each sample $\mathbf{x}_i$ such that $\mathbf{z}_i$ is disentangled. Here $K$ is the size of latent representation.
Formally, we want to separate the $K$ hidden units into $L$ groups\footnote{In this paper, we assume the value of $L$ is known. However, our solution can be easily generalized to the nonparametric case where the value $L$ can automatically adapt to the data.} where the correlation between hidden units within the same group is much larger than the one between groups. Ideally, the hidden units within the same group can form a factor.
Mathematically, we hope that the covariance of latent representation $\mathbf{z}$ has block diagonal structure, and the methods we propose in this paper highly depend on this assumption. Throughout the paper, we assume that we know the number of factors beforehand, but this assumption can be easily relaxed by using nonparametric model with Dirichless process.

In the following sections, we introduce two new models for learning disentangled representation. For the first model, we assume that we know the exact block structure of covariance matrix, while for the second model, we relax this assumption to unknown block structure.

\subsection{Learning Factors of Variation with Known Block Structure}
We assume that the structure (size) of each block is known and this makes it easier to define the corresponding block diagonal covariance matrix. Such covariance matrix can be directly applied to the prior distribution $p(\mathbf{z})$ to guide the learning procedure. In addition, we could also easily parameterize the variational posterior distribution $q_{\phi}(\mathbf{z}|\mathbf{x})$ with Gaussian distribution with block diagonal covariance matrix, which will be discussed later.

The probabilistic graphical model is shown in Figure~\ref{fig:model}.
The generative process of $p_{\theta}(\mathbf{x}|\mathbf{z})$ is governed by Bernoulli or Gaussian MLP(Multilayer Perceptron), depends on the type of input data. We use $\theta$ to denote all the parameters of this MLP which include weight matrices and bias vectors. In particular, we assume $\mathbf{x}$ is generated from Gaussian distribution with mean $\mathbf{\eta}$ and diagonal covariance $\sigma^2\mathbf{I}$, where $\mathbf{\eta}$ and $\mathbf{\sigma}$ is generated from $\mathbf{z}$ via nonlinear transformation such that $\mathbf{\eta} = \mathbf{W}_2^\top\mathbf{h}_1 + \mathbf{b}_2$ and $\log \mathbf{\sigma}^2 = \mathbf{W}_3^\top\mathbf{h}_1 + \mathbf{b}_3$, and $\mathbf{h}_1 = \mbox{tanh}(\mathbf{W}_1^\top\mathbf{z} + \mathbf{b}_1)$. We define $\theta=(\mathbf{W}_1, \mathbf{W}_2,\mathbf{W}_3, \mathbf{b}_1, \mathbf{b}_2,\mathbf{b}_3)$.

The key component of our model is the prior distribution on $p(\mathbf{z})$. We assume $p(\mathbf{z})$ follows Gaussian distribution: $p(z)\sim N(\mu, \mathbf{\Lambda})$. In order to make $\mathbf{z}$ has disentangling effects, we enforce that the covariance matrix $\mathbf{\Lambda}$ has block diagonal structure. Since we know the number of blocks as well as the size of each block, we can explicitly define the structure of $\mathbf{\Lambda}$ as follows:
\begin{equation*}
\mathbf{\Lambda} = \begin{bmatrix}
\mathbf{\Lambda}_1 & 0 & 0 \\
0 & \ddots & 0  \\
0 & 0 & \mathbf{\Lambda}_{L}
\end{bmatrix} \\
\end{equation*}
Here $\mathbf{\Lambda}_1,...\mathbf{\Lambda}_L$ is the covariance matrix for each block and each size is known. Finally, we put the Normal-Wishart prior on each $\mathbf{\Lambda}_i$ such that $\mathbf{\mathbf{\Lambda}}_{i} \sim \mathcal{W}(\mathbf{\Lambda}_{i}; \mathcal{M}_{i}, \nu_{i})$. We denote $\vartheta=(\mathcal{M}_{1}, \nu_{1},...,\mathcal{M}_{L}, \nu_{L})$

The posterior distribution $p_{\theta}(\mathbf{z}|\mathbf{x})$, unfortunately, is intractable. Thus, we use variational distribution $q_{\phi}(\mathbf{z}|\mathbf{x})$ to approximate $p_{\theta}(\mathbf{z}|\mathbf{x})$.
In this work, we assume $q_{\phi}(\mathbf{z}|\mathbf{x})$ also follows Gaussian distribution with mean $\mu'$ and covariance $\mathbf{\Lambda}'$. The definition of $\mu'$ and $\mathbf{\Lambda}'$ is similar to the definition of $\mu$, $\mathbf{\Lambda}$, both of which follow the MLP. We also enforce that $\mathbf{\Lambda}'$ has block diagonal structure, and this assumption makes the generative process different from the one used in~\cite{kingma2013auto}. In particular for each $i$, we have $\mathbf{\Lambda}_i=\mathbf{A}_i\mathbf{A}_i^{\top}$, where matrix $\mathbf{A}_i^{\top}$ is generated via nonlinear transformation of $\mathbf{x}$ such that $\mathbf{A}_i=\exp(\mathbf{W}_{5,i}^{\top}\mathbf{h}_2+\mathbf{b}_{5,i})$ for $i=1,...,L$, and $\mathbf{h}_2=tahn(\mathbf{W}_4^{\top}\mathbf{x}+\mathbf{b}_4)$. In experimental section, we also tested the model with the covariance matrix has diagonal form instead of block diagonal. We denote $\phi=(\mathbf{W}_4, \mathbf{b}_4, \{\mathbf{W}_{5,i}\}_{i=1}^{L}, \{\mathbf{b}_{5,i}\}_{i=1}^{L})$.


\subsubsection{Variational Inference}
By following the standard procedure of variational inference, we can rewrite the log-probability of observation as
\[
\log P(\mathbf{x}) = D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p_{\theta}(\mathbf{z}|\mathbf{x})) + E_{q_{\phi}(\mathbf{z}|\mathbf{x})}[\log \frac{p_{\vartheta, \theta}(\mathbf{x},\mathbf{z})}{q_{\phi}(\mathbf{z}|\mathbf{x})}]
\]
The second term provides a variational likelihood
\[
\mathcal{L} = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \log p_{\theta}(\mathbf{x}|\mathbf{z}) - D_{KL}[q_{\phi}(\mathbf{z}|\mathbf{x})||p_{\vartheta}(\mathbf{z})] := \mathcal{L}_1 + \mathcal{L}_2
\]
We can learn the model parameters by maximizing $\mathcal{L}$ using stochastic gradient descent(SGD). Gradient of $\mathcal{L}_1$ w.r.t. parameters $\theta, \phi$ can be optimized using back-propagation algorithm by following the auto-encoding variable bayes procedure~\cite{kingma2013auto}. For the KL divergence term, we can also get the closed form formulation as below
\begin{align}
\notag \mathcal{L}_2 &:= \int_{(\mu, \mathbf{\Lambda})\sim NW(\mathbf{m}, \beta, \mathcal{M}, \nu)} KL(\mathcal{N}(\eta, \mathbf{\Sigma})||\mathcal{N}(\mu, \mathbf{\Lambda}^{-1})) \ d\mu d\mathbf{\Lambda} \\
\notag				& = -\frac{1}{2}\log|\mathbf{\Sigma}| - \frac{1}{2}[\log |\mathcal{M}| + \sum_{i=1}^{d}\psi(\frac{\nu-d+i}{2})] + \frac{\nu}{2}[\mbox{tr}(\mathcal{M}\mathbf{\Sigma})] + \frac{1}{2} [\frac{d}{\beta} + \nu \mbox{tr}(\mathcal{M}(\mbox{m}-\eta)(\mbox{m}-\eta)^{\top})]
\end{align}
where $\psi(\cdot)$ is the digamma function, and $\mathbf{m}, \beta$ are the prior parameters for $\mu$. Since the KL divergence term has analytic formulation, we can optimize the related parameters using SGD.


\begin{figure}[hb]
  \centering
  \includegraphics[width=1.8in]{images/blockmodel.pdf}
  \caption[]
   {Graphical representation of our proposed block diagonal prior model. $\alpha, \sigma_d, \sigma_g, \sigma_{\eta}$ are the hyperparameters. $\{c_k\}, \{g_{kl}\}, \{\eta_{nl}\}$ are the parameters of the prior model. $\{\mathbf{z_i}\}$ is the latent representation, and $\{\mathbf{x}_i\}$ are our observations. The difference between this model and our previous model is that we add additional generative process on top of $z$. The generative process of $p(x|z)$ and its corresponding posterior $q(z|x)$ remains the same, which are parameterized by $\theta, \psi$ respectively. }
  \label{fig:blockprior}
\end{figure}

\subsection{Learning Factors of Variation with Unknown Block Structure}

So far, we assume the size of each block is known. However, this assumption can be relaxed by putting variable clustering prior~\cite{palla2012nonparametric} on $p(\mathbf{z})\in R^K$. This prior is acted as another generative process which models the correlation within the cluster and assign the cluster membership to each dimension of latent representation. After incorporation of this prior, the new probabilistic model is depicted in Figure~\ref{fig:blockprior}, and the corresponding generative process is shown as below:

\begin{enumerate}[noitemsep]
\item Draw $\pi\sim \text{Dir}(\alpha)$
\item For each latent dimension $k$ from $1$ to $K$
    \begin{enumerate}[noitemsep]
    \item Draw dimension membership $c_k\sim \text{multinomial}(\pi)$
    \end{enumerate}
\item For each dimension $k=1...K, ~~l=1...L$
    \begin{enumerate}[noitemsep]
    \item Draw factor loading variable $g_{kl}\sim N(0, \sigma_g^2)$
    \end{enumerate}
\item For each sample $n=1...N$
    \begin{enumerate}[noitemsep]
        \item For each block/cluster $l=1...L$
        \begin{enumerate}
            \item Draw latent factor $\eta_{nl}\sim N(0, \sigma_{\eta})$
        \end{enumerate}
        \item For each dimension $k=1...K$
        \begin{enumerate}
            \item Draw latent representation $z_{nk}\sim N(g_{kc_k}\eta_{nc_k}, \sigma_k^2)$
        \end{enumerate}
        \item Draw each observation $\mathbf{x}_i \sim f_{\theta}(\mathbf{z}_i)$
    \end{enumerate}
\end{enumerate}

Here, $\alpha, \sigma_g, \sigma_k, \sigma_{\eta}$ are hyperparameters that we can fix beforehand\footnote{Of course, we could also learn these parameters. But in this paper, we treat them as hyperparameters and select the optimal ones using cross validation}. $\{\eta_{nl}\}$ is the local parameters that depends on $z_n$ while $\{g_{kc_k}\}$ are the global parameters that control the dimension membership. Based on this variable clustering prior model, the covariance of $\text{cov}(z_{nk}, z_{nk'}|\{g_{kl}\}, \{c_k\})$ as below
\[ \text{cov}(z_{nk}, z_{nk'}|\{g_{kl}\}, \{c_k\}) = \left\{
  \begin{array}{l l}
    \sigma_{\eta}^2g_{kc_k}g_{k'c_{k'}}+\sigma_d^2\delta_{kk'} & \quad \text{if $c_k=c_{k'}$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.\]
This explicitly defines the block diagonal covariance structure, and enables the model to automatically learn the structure (size) of each block.  Besides the change of prior on $p(\mathbf{z})$, remaining stays the same. As before, we model $p_{\theta}(\mathbf{x}|\mathbf{z})$ using Gaussian distribution whose parameters are generated by MLP, and the variational distribution $q_{\phi}(\mathbf{z}|\mathbf{x})$ is also modeled by Gaussian distribution but with diagonal covariance matrix.

\subsubsection{Inference}
The learning for current model is more complicated than previous ones, which requires the sampling steps for parameters of prior model. We use the gibbs sampler to compute the expected values of $\mathbf{G}, \mathbf{C}$. After we compute those values, the conditional distribution $p(\mathbf{z}|\mathbf{G},\mathbf{C})$ can be obtained as the closed form formulation, which is made as the input to KL divergence term in the variational likelihood. The learning for the remaining part remains the same. The overall learning procedure is shown in Algorithm~\ref{alg:model}.

\paragraph{Sampling $\mathbf{G}$} $p(g_{kl}|\mathbf{C},\mathbf{G},\mathbf{\eta}, \mathbf{Z},\sigma_g)\sim \mathcal{N}(\mu_{g_{kl}}^*, \Lambda_{g_{kl}}^*)$, where
\[ \mu_{g_{kl}}^*, \Lambda_{g_{kl}}^* =
  \begin{cases}
     \mu_g, \sigma_g^{-2}      & \text{if} ~~c_{kl} = 0\\
    \left(\frac{\mu_g}{\sigma_g^2}+\frac{1}{\sigma_d^2}\sum_{n=1}^{N}\eta_{nl}z_{nk}\right)\Lambda_{g_{kl}}^{-1}, \frac{1}{\sigma_d^2}\left(\frac{\sigma_d^2}{\sigma_g^2}+\sum_{n=1}^{N}\eta_{kl}^2\right)  & \text{if}~~ c_{kl} = 1\\
  \end{cases}
\]
\paragraph{Sampling $\mathbf{\eta}$} $p(\eta_{nl}|\mathbf{C},\mathbf{G},\sigma_{\eta})\sim \mathcal{N}(\mu_{\eta_l}^*, \Lambda_{\eta_{l}}^*)$, where
\[
\mu_{\eta_l}^*, \Lambda_{\eta_{l}}^*={\Lambda_{\eta_l}^*}^{-1}\left(\frac{\mu_\eta}{\sigma_{\eta}^2}+\frac{\sum_{k=1}^{K}g_{kl}c_{kl}z_{nk}}{\sigma_d^2}\right),
\frac{1}{\sigma_{\eta}^2}+\frac{\sum_{k=1}^{K}g_{kl}^2c_{kl}^2}{\sigma_d^2}
\]
\paragraph{Sampling $\mathbf{C}$} $p(C_{kl}=1|Z, X, G) \sim \Lambda_{g_{kl}}^{-1/2}\exp(\frac{\Lambda_{g_{kl}}^{\ast}{\mu_{g_{kl}}^{\ast}}^2}{2})P(C_{dk}=1|\alpha)$ 

\begin{algorithm}[t]
\caption{Learning procedure for our model with unknown block structure}\label{alg}
\begin{algorithmic}[1]
\STATE initialize $\sigma_k, \sigma_g, \sigma_{\eta}$
\STATE initialize $\pi, \{c_d\}, \{g_{dk}\}$ by drawing from Gaussian distributions.
\FOR{ epoch=1...Max\_{epoch} (one pass over the whole training set)}
\FOR {each mini-batch $D$}
\STATE Run forward propagation on $D$ and compute $Z$.
\STATE Run gibbs sampler to compute $p(\eta_{nl}|\{z_{nk}\}, \{c_k\}, \{g_{kl}\}), ~~p(\{g_{kl}\}|\{z_{nk}\},\{c_k\}, \{\eta_{nl}\})$\\
$p(\{c_k\}|\{z_{nk}\},\{g_{kl}\}, \theta, \{\eta_{nl}\}), ~~p(\pi|\alpha,\{c_k\})$, and update using averaged samples.
\STATE Compute $p(z|\{g_{kl}^*\},\{c_k^*\})$
\STATE Learn the model parameters $\theta$ and $\phi$ using SGD (same as before).
\ENDFOR
\ENDFOR
\end{algorithmic}
\label{alg:model}
\end{algorithm}














\iffalse

\paragraph{Generative model for $p(\mathbf{x}|\mathbf{z};\theta)$}  We assume $\theta=(\mathbf{W}_4^T, \mathbf{W}_5^T,\mathbf{W}_6^T, \mathbf{b}_4, \mathbf{b}_5,\mathbf{b}_6)$

\begin{align}
& p(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\eta, \sigma^2\mathbf{I}) \\
& \eta = \mathbf{W}_5^T\mathbf{h}_2 + \mathbf{b}_5 \\
& \log \sigma^2 = \mathbf{W}_6^T\mathbf{h}_2 + \mathbf{b}_6 \\
& \mathbf{h}_2 = \mbox{tanh}(\mathbf{W}_4^T\mathbf{z} + \mathbf{b}_4)
\end{align}


\paragraph{Prior model for $p(\mathbf{z})$}
We introduce a block diagonal regularizer by modeling the prior distribution of $\mathbf{z}$ as block diagonal Gaussian. We assume $\alpha=(\mathcal{M}_{1}, \nu_{1},...,\mathcal{M}_{k},\nu_{k})$
\begin{align}
& p(\mathbf{z}| \mathbf{\Lambda}) = \mathcal{N}(\mathbf{m},\mathbf{\Lambda)}, \mbox{where} \nonumber\\
& \mathbf{\Lambda} = \begin{bmatrix}
\mathbf{\Lambda}_1 & 0 & 0 \\
0 & \ddots & 0  \\
0 & 0 & \mathbf{\Lambda}_{K}
\end{bmatrix} \\
& \mathbf{\Lambda}_{i} \sim \mathcal{W}(\mathbf{\Lambda}_{i}; \mathcal{M}_{i}, \nu_{i})\\
& p_{\alpha}(\mathbf{z}) = \prod_{k=1}^{K}  \int N(\mathbf{m}_k, \mathbf{\Lambda}_k) \mathcal{W}(\mathbf{\Lambda}_{k}; \mathcal{M}_{k}, \nu_{k}) d\mathbf{\Lambda}
\end{align}

\paragraph{Posterior model for $q(\mathbf{z}|\mathbf{x};\phi)$}
The approximate posterior model $q(\mathbf{z}|\mathbf{x};\phi)$ is
\begin{align}
& q_{\phi}(\mathbf{z}|\mathbf{x})=N(\mu', \mathbf{\Lambda}) \\
& \mu' = \mathbf{W}_2^T h_1 + \mathbf{b}_2 \\
&\mathbf{\Lambda} = \begin{bmatrix}
\mathbf{\Lambda}_1 & 0 & 0 \\
0 & \ddots & 0  \\
0 & 0 & \mathbf{\Lambda}_{K}
\end{bmatrix} \\
& \mathbf{\Lambda_k} = \mathbf{A}_k\mathbf{A}_k^{\top}\\
& \mathbf{A}_k= \exp(\mathbf{W}_{3k}^{\top}h_1 + \mathbf{b}_{3k})\\
%& \sigma(\mathbf{x}, \mathbf{W}_1, \mathbf{W}_3) = A(\mathbf{x},\mathbf{W}_1,\mathbf{W}_3)\cdot A^T(\mathbf{x},\mathbf{W}_1,\mathbf{W}_3)\\
& h_1 = \mbox{tahn}(\mathbf{W}_1^T\mathbf{x} + \mathbf{b}_1)
\end{align}


\subsubsection{Variational Inference}
By following the standard procedure of variational inference, we can rewrite log-probability of observation as
\begin{align}
\log P(\mathbf{x}) = D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p_{\theta}(\mathbf{z}|\mathbf{x})) + E_{q_{\phi}(\mathbf{z}|\mathbf{x})}[\log \frac{p_{\alpha, \theta}(\mathbf{x},\mathbf{z})}{q_{\phi}(\mathbf{z}|\mathbf{x})}]
\end{align}
The second term provides a variational likelihood
\begin{align}
\mathcal{L} = E_{q_{\phi}(\mathbf{z}|\mathbf{x})} \log p_{\theta}(\mathbf{x}|\mathbf{z}) - D_{KL}[q_{\phi}(\mathbf{z}|\mathbf{x})||p_{\alpha}(\mathbf{z})] := \mathcal{L}_1 + \mathcal{L}_2
\end{align}

We can learn the model by maximizing $\mathcal{L}$ using SGD. Gradient of $\mathcal{L}_1$ w.r.t. parameters $\theta, \phi$ can be calculated using back-propagation algorithm, by following the auto-encoding variable bayes procedure. For the KL divergence term, we could also get the closed form formulation, which can be optimized SGD as well.


\begin{figure}
        \centering
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{model2}
                \label{fig:gull}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.37\textwidth}
                \includegraphics[width=\textwidth]{model1}
                \label{fig:tiger}
        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
        \caption{(a) Graph representation of proposed model. The only observation here is the input $x$. $\Lambda, m$ are modeled by normal-wishart prior. (b) The detailed flow of transforming $x$ to reconstruction $y$. The nonlinear transformation function are denoted on the right hand side}
\end{figure}


\subsubsection{Optimizing likelihood term}
Figure 2-(b) shows the detailed auto-encoding procedure for likelihood term. It shows the case where we have two blocks. Below is the list of transformations denoted by this figure.
\begin{align}
\mathbf{h}_1&=tanh(\mathbf{W}_1^{\top}\mathbf{x})\\
\mathbf{\mu}_1&=\mathbf{W}_{21}^{\top}\mathbf{h}_1\\
\mathbf{\mu}_2&=\mathbf{W}_{22}^{\top}\mathbf{h}_1\\
\log \mathbf{A}_1 &= \mathbf{W}_{31}^{\top}\mathbf{h}_1\\
\log \mathbf{A}_2 &= \mathbf{W}_{32}^{\top}\mathbf{h}_1\\
\mathbf{\mu} &= [\mathbf{\mu}_1, \mathbf{\mu}_2] \\
\mathbf{A} &= [\mathbf{A}_1, 0; 0, \mathbf{A}_2] \\
\mathbf{z}^{(l)}&=\mathbf{\mu} + \mathbf{A} \mathbf{\epsilon}^{(l)}, ~~~\mathbf{\epsilon}^{(l)} \sim \mathcal{N}(0, \mathbf{I})\\
\mathbf{h}_2^{(l)} &= tanh(\mathbf{W}_4^{\top}\mathbf{z}^{(l)})\\
\mathbf{y}^{(l)} &= \sigma(\mathbf{W}_5^{\top}\mathbf{h}_2^{(l)})
\end{align}


If our input data is the binary images (i.e MNIST), then the likelihod  thus the likelihood term for specific sample $x$ can be written as (if input data is continuous images, we use $l_2$ reconstruction loss) :
\[
\mathcal{L}^{(l)} = \sum_{d=1}^{D_0}x_d\log y_d^{(l)}+(1-x_d)\log(1-y_d^{(l)})
\]
During the learning procedure, we need to generate multiple latent representations by combining different random noise with the given data $x$. This is the reason why we use superscript $(l)$ to denote the specific version of latent representation for input $x$.

Given this likelihood term, the gradient w.r.t each hidden variable can be written as:
\begin{align}
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_3} &= \mathbf{x}-\mathbf{y}^{(l)} \\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_2} &= (\mathbf{W}_5 \frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_3}) \odot (1-\mathbf{h}^2)\\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}} &= \mathbf{W}_4 \frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_2}\\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{A}} &= \frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}} {\mathbf{\epsilon}^{(l)}}^{\top}, ~~~  \mathbf{\epsilon}^{(l)} \sim \mathcal{N}(0, \mathbf{I}) \\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_1}&=(W_2\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{\mu}}+W_3(\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{A}})\odot \mathbf{A}) \odot (1-{\mathbf{h}_1}^2)\\
\frac{\partial L^{1}}{\partial \mu}&= \frac{\partial L^{(1)}}{\partial z}
\end{align}

Finally, the gradient for each parameter is:
\begin{align}
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{W}_5}&=\mathbf{h}_2^{(l)} (\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_3})^{\top} \\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{W}_4} &= \mathbf{z}(\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_2})^{\top}\\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{W}_3} &= \mathbf{h}_1 (\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{A}}\odot \mathbf{A} )^{\top}\\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{W}_2} &= \mathbf{h}_1 (\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{\mu}})^{\top}\\
\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{W}_1} &= \mathbf{x}(\frac{\partial \mathcal{L}^{(l)}}{\partial \mathbf{z}_1})^{\top}
\end{align}


\subsubsection{Optimizing KL divergence term}
We put the normal-wishart prior on the parameters of the normal distributions. This is equivalent to integrating out all possible normal distribution parameters. The derivation of KL divergence term is shown as below:

\begin{align}
\notag \mathcal{L}_{KL} &:= \int_{(\mu, \Lambda)\sim NW(\mathbf{m}, \beta, W, \nu)} KL(\mathcal{N}(\eta, \Sigma)||\mathcal{N}(\mu, \Lambda^{-1})) \ d\mu d\Lambda \\
\notag				& :=  \int_{(\mu, \Lambda)\sim NW(\mathbf{m}, \beta, W, \nu)}  \frac{1}{2}[-\log(|\Lambda|) - \log(|\Sigma|) -d + \mbox{tr}(\Lambda\Sigma) + (\mu-\eta)^T\Lambda(\mu-\eta)]\ d\mu d\Lambda \\
\notag				& = -\frac{1}{2}\log|\Sigma| - \frac{1}{2}E_{\mu, \Lambda}[\log |\Lambda|] + \frac{1}{2}E_{\mu, \Lambda} [\mbox{tr}(\Lambda\Sigma)] + \frac{1}{2} E_{\mu, \Lambda}[(\eta-\mu)^T\Lambda(\eta-\mu)]\\
\notag				& = -\frac{1}{2}\log|\Sigma| - \frac{1}{2}[\log |W| + \sum_{i=1}^{d}\psi(\frac{\nu-d+i}{2})] + \frac{\nu}{2}[\mbox{tr}(W\Sigma)] + \frac{1}{2} [\frac{d}{\beta} + \nu \mbox{tr}(W(\mbox{m}-\eta)(\mbox{m}-\eta)^T)]
\end{align}

where $\psi(\cdot)$ is the digamma function. Discarding the terms irrelevant to $\eta, \Sigma$, we can obtain a simplified formulation of KL divergence part in objective function:
\begin{align}
\label{KLW} \mathcal{L}_2 := \frac{1}{2} \sum_{n=1}^{N} \log|\Sigma| - \mbox{tr}(\nu W \Sigma ) - (\mathbf{m}-\eta)^T \nu W (\mathbf{m}-\eta)
\end{align}





\subsection{Putting block diagonal prior on $p(\mathbf{z})$}

\begin{figure}[hb]
  \centering
  \includegraphics[width=2in]{blockmodel.png}
  \caption[]
   {Graphical representation of our proposed block diagonal prior model. $\alpha, \sigma_d, \sigma_g, \sigma_{\eta}$ are the hyperparameters. $\{c_d\}, \{g_{dk}\}, \{\eta_{nk}\}$ are the parameters of the prior model. $\{\mathbf{z_i}\}$ is the latent representation, and $\{\mathbf{x}_i\}$ are our observations. The difference between this model and our previous model is that we add additional generative process on top of $z$. The generative process of $p(x|z)$ and its corresponding posterior $q(z|x)$ remains the same, which are parameterized by $\theta, \psi$ respectively.}
\end{figure}


For now, we assume the number of blocks $K$, is known. But this can be easily generalized to the nonparametric case, by using Dirichlet process. The core idea of block prior is given in~\cite{palla2012nonparametric} and the draft by Ren and Emily Fox.

By incorporating the block diagonal prior, the generative process of our model is given below:

\begin{enumerate}[noitemsep]
\item Draw $\pi\sim Dir(\alpha)$
\item For each latent dimension $d$ from $1$ to $D$
\item $~~~~$Draw dimension membership $c_d\sim multinomial(\pi)$
\item For $d=1...D, ~~k=1...K$
\item $~~~~$Draw factor loading variable $g_{dk}\sim N(0, \sigma_g^2)$
\item For each sample $n=1...N$
\item $~~~~$For each block/cluster $k=1...K$
\item $~~~~~~~~$Draw latent factor $\eta_{nk}\sim N(0, \sigma_{\eta})$
\item $~~~~$For $d=1...D$
\item $~~~~~~~~$Draw latent representation $z_{nd}\sim N(g_{dc_d}\eta_{nc_d}, \sigma_d^2)$
\item $~~~~$Draw $x_i \sim f_{\theta}(z_i)$
\end{enumerate}

Based on this block diagonal prior model, we could get

\[ cov(z_{nd}, z_{nd'}|\{g_{dk}\}, \{c_d\}) = \left\{
  \begin{array}{l l}
    \sigma_{\eta}^2g_{dc_d}g_{d'c_{d'}}+\sigma_d^2\delta_{dd'} & \quad \text{if $c_d=c_{d'}$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.\]

Please note that when we draw $x_i$ from $z_i$, we use MLP as before. Also, we model the posterior $p(z|x)$ as diagonal covariance matrix. (we could also use full covariance matrix). We treat $\sigma_d, \sigma_{\eta}, \sigma_g$ as hyperparameters, which we fix beforehand. (of course, we can also learn these parameters as well).

\section{Learning the model}
The learning procedure is in Algorithm 1. Basically, there are two large components in our model, one is our previous MLP model (similar to Max Welling's formulation), and the other one is the block diagonal prior we impose. For the latter one, we use gibbs sampling. The latent representation $\mathbf{Z}$ is the bridge between these two components. And we alternatively optimize these two components.


\begin{algorithm}[t]
\caption{Learning procedure for our model}\label{alg}
\begin{algorithmic}[1]
\STATE initialize $\sigma_d, \sigma_g, \sigma_{\eta}$
\STATE initialize $\pi, \{c_d\}, \{g_{dk}\}$ by drawing from corresponding distributions.
\FOR{iteration $\text{itr}=1...\text{Max_itr}$ (one pass over the whole training set)}
\FOR {each mini-batch $D$}
\STATE Run forward propagation on $D$ and compute $Z$.
\STATE Run gibbs sampler to compute $p(\eta_{nl}|\{z_{nk}\}, \{c_k\}, \{g_{kl}\}), ~~p(\{g_{kl}\}|\{z_{nk}\},\{c_k\}, \{\eta_{nl}\})$\\
$p(\{c_k\}|\{z_{nk}\},\{g_{kl}\}, \theta, \{\eta_{nl}\}), ~~p(\pi|\alpha,\{c_k\})$, and update using averaged samples.
\STATE Compute $p(z|\{g_{kl}\},\{c_k\},\sigma_k)$
\STATE Learn the model parameters $\theta$ and $\phi$ using backpropagation (as we did before).
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\fi
